{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some ways to create a bag of words\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install gensim\n",
    "project_base=\"/home/username/haiku/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = wordnet.synsets(\"plant_organ\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo = wordnet.synset(\"plant_organ.n.01\").hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def most_similar(word, topn=5):\n",
    "    word = nlp.vocab[str(word)]\n",
    "    queries = [\n",
    "        w for w in word.vocab \n",
    "        if w.is_lower == word.is_lower and w.prob >= -15 and np.count_nonzero(w.vector)\n",
    "    ]\n",
    "\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return [(w.lower_,w.similarity(word)) for w in by_similarity[:topn+1] if w.lower_ != word.lower_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(\"sky\", topn=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ss in wordnet.synsets('happiness'):\n",
    "    print(ss.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonyms(word, pos_tag):\n",
    "  return list(\n",
    "    {\n",
    "      lemma.replace(\"_\",\" \").replace(\"-\",\" \") for synset in wordnet.synsets(\n",
    "        _clean_word(word),\n",
    "        pos_tag,\n",
    "      ) for lemma in synset.lemma_names()\n",
    "    }\n",
    "  )\n",
    "\n",
    "from string import punctuation\n",
    "def _clean_word(word):\n",
    "  return word.lower().strip(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms(\"sky\",\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deadly-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import gensim\n",
    "model = gensim.models.Word2Vec(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interested-finder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-d14e1c37ded7>:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  model.most_similar(positive=['chest'], topn = 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('shoulders', 0.9764670729637146),\n",
       " ('shoulder', 0.9759306907653809),\n",
       " ('sky', 0.9739558100700378),\n",
       " ('wagon', 0.9723237752914429),\n",
       " ('seat', 0.9685256481170654),\n",
       " ('knee', 0.9651641845703125),\n",
       " ('cheek', 0.9644531607627869),\n",
       " ('nose', 0.9642648100852966),\n",
       " ('pocket', 0.9636731147766113),\n",
       " ('mirror', 0.9618158340454102)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['chest'], topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faced-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.data import find\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quantitative-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('snakes', 0.7513571977615356),\n",
       " ('python', 0.660629153251648),\n",
       " ('cobra', 0.648840606212616),\n",
       " ('rattlesnake', 0.6464701890945435),\n",
       " ('serpent', 0.6409705877304077),\n",
       " ('alligator', 0.6349284052848816),\n",
       " ('spider', 0.6340566873550415),\n",
       " ('crocodile', 0.6303909420967102),\n",
       " ('monkey', 0.6056618690490723),\n",
       " ('critter', 0.5975096225738525),\n",
       " ('toad', 0.5919240713119507),\n",
       " ('lizards', 0.5737593173980713),\n",
       " ('turtle', 0.5653021335601807),\n",
       " ('rattlesnakes', 0.5651679635047913),\n",
       " ('rat', 0.562048614025116),\n",
       " ('creature', 0.5596329569816589),\n",
       " ('possum', 0.5569279193878174),\n",
       " ('constrictor', 0.5555417537689209),\n",
       " ('serpents', 0.5516219735145569),\n",
       " ('fox', 0.5493873953819275),\n",
       " ('cottonmouth', 0.5457687377929688),\n",
       " ('rattlers', 0.5444181561470032),\n",
       " ('frog', 0.5431819558143616),\n",
       " ('rabbit', 0.5367448329925537),\n",
       " ('rodent', 0.5361548662185669),\n",
       " ('anaconda', 0.5355017185211182),\n",
       " ('rattler', 0.5352160930633545),\n",
       " ('cat', 0.5259495377540588),\n",
       " ('tortoise', 0.5164839029312134),\n",
       " ('squirrel', 0.5162323117256165),\n",
       " ('elephant', 0.5066288709640503),\n",
       " ('coyote', 0.5054575800895691),\n",
       " ('owl', 0.5039119124412537),\n",
       " ('boa', 0.4969353675842285),\n",
       " ('tiger', 0.48850712180137634),\n",
       " ('turtles', 0.48732227087020874),\n",
       " ('kitten', 0.4834090769290924),\n",
       " ('otter', 0.4829707145690918),\n",
       " ('constrictors', 0.4820745587348938),\n",
       " ('critters', 0.4818723499774933)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['snake'], topn = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "through-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.most_similar(positive=['volcano'], topn = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stable-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for re in res:\n",
    "    words.append(re[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "annual-genetics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eruption\n",
      "lava\n",
      "volcanic\n",
      "volcanos\n",
      "crater\n",
      "glacier\n",
      "ash\n",
      "plume\n",
      "mountain\n",
      "earthquakes\n",
      "geysers\n",
      "tremor\n",
      "Krakatoa\n",
      "glaciers\n",
      "cave\n",
      "ridge\n",
      "landslides\n",
      "erupting\n",
      "tsunami\n",
      "slopes\n",
      "quake\n",
      "earthquake\n",
      "typhoon\n",
      "Iceland\n",
      "comet\n",
      "mountainside\n",
      "meteor\n",
      "massifs\n",
      "archipelago\n",
      "Himalayas\n",
      "undersea\n",
      "waterfall\n",
      "sunspot\n",
      "Kamchatka\n",
      "mountains\n",
      "asteroid\n",
      "Honshu\n",
      "canyon\n",
      "Sumatra\n",
      "Pompeii\n",
      "craters\n",
      "ocean\n",
      "equatorial\n",
      "seaquake\n",
      "mountainsides\n",
      "Empedocles\n",
      "Tikopia\n",
      "caves\n",
      "avalanche\n",
      "meteorite\n",
      "nebula\n",
      "dome\n",
      "epicenter\n",
      "geologists\n",
      "Celebes\n",
      "meteors\n",
      "sea\n",
      "tropics\n",
      "Antarctica\n",
      "altitude\n",
      "tropical\n",
      "Tsunami\n",
      "brushfire\n",
      "slope\n",
      "evacuation\n",
      "telescope\n",
      "mud\n",
      "waterfalls\n",
      "floe\n",
      "ridges\n",
      "seismographs\n",
      "foothills\n",
      "mountainous\n",
      "comets\n",
      "islands\n",
      "seismological\n",
      "cavern\n",
      "cliff\n",
      "hillside\n",
      "storm\n",
      "island\n",
      "radioactivity\n",
      "planetoid\n",
      "boulders\n",
      "boulder\n",
      "quetzal\n",
      "foothill\n",
      "spewing\n",
      "seismic\n",
      "Moluccas\n",
      "geological\n",
      "kiloton\n",
      "chimney\n",
      "gusher\n",
      "gullies\n",
      "cliffs\n",
      "erupt\n",
      "equator\n",
      "storms\n",
      "hurricane\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERPLEXITY ANALYSIS FOLLOWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "other-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "# used for unseen words in training vocabularies\n",
    "UNK = None\n",
    "# sentence start and end\n",
    "SENTENCE_START = \"<s>\"\n",
    "SENTENCE_END = \"</s>\"\n",
    "\n",
    "def read_sentences_from_file(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return [re.split(\"\\s+\", line.rstrip('\\n')) for line in f]\n",
    "\n",
    "class UnigramLanguageModel:\n",
    "    def __init__(self, sentences, smoothing=False):\n",
    "        self.unigram_frequencies = dict()\n",
    "        self.corpus_length = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                self.unigram_frequencies[word] = self.unigram_frequencies.get(word, 0) + 1\n",
    "                if word != SENTENCE_START and word != SENTENCE_END:\n",
    "                    self.corpus_length += 1\n",
    "        # subtract 2 because unigram_frequencies dictionary contains values for SENTENCE_START and SENTENCE_END\n",
    "        self.unique_words = len(self.unigram_frequencies) - 2\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def calculate_unigram_probability(self, word):\n",
    "            word_probability_numerator = self.unigram_frequencies.get(word, 0)\n",
    "            word_probability_denominator = self.corpus_length\n",
    "            if self.smoothing:\n",
    "                word_probability_numerator += 1\n",
    "                # add one more to total number of seen unique words for UNK - unseen events\n",
    "                word_probability_denominator += self.unique_words + 1\n",
    "            return float(word_probability_numerator) / float(word_probability_denominator)\n",
    "\n",
    "    def calculate_sentence_probability(self, sentence, normalize_probability=True):\n",
    "        sentence_probability_log_sum = 0\n",
    "        for word in sentence:\n",
    "            if word != SENTENCE_START and word != SENTENCE_END:\n",
    "                word_probability = self.calculate_unigram_probability(word)\n",
    "                sentence_probability_log_sum += math.log(word_probability, 2)\n",
    "        return math.pow(2, sentence_probability_log_sum) if normalize_probability else sentence_probability_log_sum                \n",
    "\n",
    "    def sorted_vocabulary(self):\n",
    "        full_vocab = list(self.unigram_frequencies.keys())\n",
    "        full_vocab.remove(SENTENCE_START)\n",
    "        full_vocab.remove(SENTENCE_END)\n",
    "        full_vocab.sort()\n",
    "        full_vocab.append(UNK)\n",
    "        full_vocab.append(SENTENCE_START)\n",
    "        full_vocab.append(SENTENCE_END)\n",
    "        return full_vocab\n",
    "\n",
    "class BigramLanguageModel(UnigramLanguageModel):\n",
    "    def __init__(self, sentences, smoothing=False):\n",
    "        UnigramLanguageModel.__init__(self, sentences, smoothing)\n",
    "        self.bigram_frequencies = dict()\n",
    "        self.unique_bigrams = set()\n",
    "        for sentence in sentences:\n",
    "            previous_word = None\n",
    "            for word in sentence:\n",
    "                if previous_word != None:\n",
    "                    self.bigram_frequencies[(previous_word, word)] = self.bigram_frequencies.get((previous_word, word),\n",
    "                                                                                                 0) + 1\n",
    "                    if previous_word != SENTENCE_START and word != SENTENCE_END:\n",
    "                        self.unique_bigrams.add((previous_word, word))\n",
    "                previous_word = word\n",
    "        self.unique__bigram_words = len(self.unigram_frequencies)\n",
    "\n",
    "    def calculate_bigram_probabilty(self, previous_word, word):\n",
    "        bigram_word_probability_numerator = self.bigram_frequencies.get((previous_word, word), 0)\n",
    "        bigram_word_probability_denominator = self.unigram_frequencies.get(previous_word, 0)\n",
    "        if self.smoothing:\n",
    "            bigram_word_probability_numerator += 1\n",
    "            bigram_word_probability_denominator += self.unique__bigram_words\n",
    "        return 0.0 if bigram_word_probability_numerator == 0 or bigram_word_probability_denominator == 0 else float(\n",
    "            bigram_word_probability_numerator) / float(bigram_word_probability_denominator)\n",
    "\n",
    "    def calculate_bigram_sentence_probability(self, sentence, normalize_probability=True):\n",
    "        bigram_sentence_probability_log_sum = 0\n",
    "        previous_word = None\n",
    "        for word in sentence:\n",
    "            if previous_word != None:\n",
    "                bigram_word_probability = self.calculate_bigram_probabilty(previous_word, word)\n",
    "                bigram_sentence_probability_log_sum += math.log(bigram_word_probability, 2)\n",
    "            previous_word = word\n",
    "        return math.pow(2,\n",
    "                        bigram_sentence_probability_log_sum) if normalize_probability else bigram_sentence_probability_log_sum\n",
    "\n",
    "# calculate number of unigrams & bigrams\n",
    "def calculate_number_of_unigrams(sentences):\n",
    "    unigram_count = 0\n",
    "    for sentence in sentences:\n",
    "        # remove two for <s> and </s>\n",
    "        unigram_count += len(sentence) - 2\n",
    "    return unigram_count\n",
    "\n",
    "def calculate_number_of_bigrams(sentences):\n",
    "        bigram_count = 0\n",
    "        for sentence in sentences:\n",
    "            # remove one for number of bigrams in sentence\n",
    "            bigram_count += len(sentence) - 1\n",
    "        return bigram_count\n",
    "\n",
    "# print unigram and bigram probs\n",
    "def print_unigram_probs(sorted_vocab_keys, model):\n",
    "    for vocab_key in sorted_vocab_keys:\n",
    "        if vocab_key != SENTENCE_START and vocab_key != SENTENCE_END:\n",
    "            print(\"{}: {}\".format(vocab_key if vocab_key != UNK else \"UNK\",\n",
    "                                       model.calculate_unigram_probability(vocab_key)), end=\" \")\n",
    "    print(\"\")\n",
    "\n",
    "def print_bigram_probs(sorted_vocab_keys, model):\n",
    "    print(\"\\t\\t\", end=\"\")\n",
    "    for vocab_key in sorted_vocab_keys:\n",
    "        if vocab_key != SENTENCE_START:\n",
    "            print(vocab_key if vocab_key != UNK else \"UNK\", end=\"\\t\\t\")\n",
    "    print(\"\")\n",
    "    for vocab_key in sorted_vocab_keys:\n",
    "        if vocab_key != SENTENCE_END:\n",
    "            print(vocab_key if vocab_key != UNK else \"UNK\", end=\"\\t\\t\")\n",
    "            for vocab_key_second in sorted_vocab_keys:\n",
    "                if vocab_key_second != SENTENCE_START:\n",
    "                    print(\"{0:.5f}\".format(model.calculate_bigram_probabilty(vocab_key, vocab_key_second)), end=\"\\t\\t\")\n",
    "            print(\"\")\n",
    "    print(\"\")\n",
    "\n",
    "# calculate perplexty\n",
    "def calculate_unigram_perplexity(model, sentences):\n",
    "    unigram_count = calculate_number_of_unigrams(sentences)\n",
    "    sentence_probability_log_sum = 0\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            sentence_probability_log_sum -= math.log(model.calculate_sentence_probability(sentence), 2)\n",
    "        except:\n",
    "            sentence_probability_log_sum -= float('-inf')\n",
    "    return math.pow(2, sentence_probability_log_sum / unigram_count)\n",
    "\n",
    "def calculate_bigram_perplexity(model, sentences):\n",
    "    number_of_bigrams = calculate_number_of_bigrams(sentences)\n",
    "    bigram_sentence_probability_log_sum = 0\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            bigram_sentence_probability_log_sum -= math.log(model.calculate_bigram_sentence_probability(sentence), 2)\n",
    "        except:\n",
    "            bigram_sentence_probability_log_sum -= float('-inf')\n",
    "    return math.pow(2, bigram_sentence_probability_log_sum / number_of_bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "industrial-tractor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERPLEXITY == \n",
      "unigram:  545.1452097219028\n",
      "bigram:  1433.5895234684365\n",
      "\n",
      "PERPLEXITY  gendata== \n",
      "unigram:  4848426724.64201\n",
      "bigram:  14007.189093549354\n"
     ]
    }
   ],
   "source": [
    "dataset = read_sentences_from_file(project_base+\"/tests/original_dataset.txt\")\n",
    "dataset_test = re.split(\"\\s+\", \"<s> pigeons in the hollow + i return fire + to birdsong </s>\".rstrip('\\n')) #read_sentences_from_file(project_base+\"/tests/one.txt\")\n",
    "\n",
    "#dataset_model_unsmoothed = BigramLanguageModel(dataset)\n",
    "dataset_model_smoothed = BigramLanguageModel(dataset, smoothing=True)\n",
    "\n",
    "#sorted_vocab_keys = dataset_model_unsmoothed.sorted_vocabulary()\n",
    "\n",
    "print(\"PERPLEXITY == \")\n",
    "\n",
    "print(\"unigram: \", calculate_unigram_perplexity(dataset_model_smoothed, dataset))\n",
    "print(\"bigram: \", calculate_bigram_perplexity(dataset_model_smoothed, dataset))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"PERPLEXITY  gendata== \")\n",
    "print(\"unigram: \", calculate_unigram_perplexity(dataset_model_smoothed, dataset_test))\n",
    "print(\"bigram: \", calculate_bigram_perplexity(dataset_model_smoothed, dataset_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "determined-furniture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram:  14007.189093549354\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "likely-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AMT DATA PROCESSING FOLLOWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import csv\n",
    "res = []\n",
    "with open(project_base+'/bare_3.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for row in reader:\n",
    "        np = numpy.asarray(row).astype(numpy.float)\n",
    "        res.append(numpy.reshape(np, (20, 3)))\n",
    "        #resh = numpy.reshape(np, (20, 3))\n",
    "        \n",
    "mean = numpy.mean(res, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.savetxt(project_base+\"/out3.csv\", mean, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUANTITATIVE ANALYSIS OF DATASETS FOLLOWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import base64\n",
    "\n",
    "datasets = ['guttenberg', 'sballas', 'tempslibre']\n",
    "\n",
    "for dataset in datasets:\n",
    "    with open(project_base+'/dataset/'+dataset+'.csv', 'rb') as file:\n",
    "        vocabulary = []\n",
    "        i = 0\n",
    "        for line in file:\n",
    "            if isinstance(line, (bytes, bytearray)):\n",
    "                line = line.decode()\n",
    "            line = re.sub(\" , [0-2][0-9]:[0-5][0-9]\", \"\", str(line))\n",
    "            line = re.sub(\"[,|!|.|?|\\\"]\", \" \", line)\n",
    "            line = line.replace(\"_\",\"\")\n",
    "            line = line.replace(\"'\",\" \")\n",
    "            line = line.replace(\";\",\"\")\n",
    "            line = line.replace(\"-\",\"\")\n",
    "            line = line.replace(\"—\",\"\")\n",
    "            line = line.replace(\"~\",\"\")\n",
    "            line = line.replace(\"(\",\"\")\n",
    "            line = line.replace(\")\",\"\")\n",
    "            words = [w.lower() for w in line.split()]\n",
    "            for word in words:\n",
    "                if word in vocabulary:\n",
    "                    break\n",
    "                else:\n",
    "                    vocabulary.append(word)\n",
    "                    i = i +1\n",
    "    print (dataset+\": \" +str(i))\n",
    "    #print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import base64\n",
    "import nltk\n",
    "\n",
    "datasets = ['guttenberg', 'sballas', 'tempslibre']\n",
    "#datasets = ['sballas']\n",
    "\n",
    "for dataset in datasets:\n",
    "    with open(project_base+'/dataset/'+dataset+'.csv', 'rb') as file:\n",
    "        corpus = \"\"\n",
    "        for line in file:\n",
    "            if isinstance(line, (bytes, bytearray)):\n",
    "                line = line.decode()\n",
    "            line = re.sub(\" , [0-2][0-9]:[0-5][0-9]\", \"\", str(line))\n",
    "            line = re.sub(\"[,|!|.|?|\\\"]\", \" \", line)\n",
    "            line = line.replace(\"_\",\"\")\n",
    "            line = line.replace(\"'\",\" \")\n",
    "            line = line.replace(\";\",\"\")\n",
    "            line = line.replace(\"-\",\"\")\n",
    "            line = line.replace(\"—\",\"\")\n",
    "            line = line.replace(\"~\",\"\")\n",
    "            line = line.replace(\"(\",\"\")\n",
    "            line = line.replace(\")\",\"\")        \n",
    "            corpus = corpus + \" \" + line\n",
    "            #print()\n",
    "        text = nltk.word_tokenize(corpus)\n",
    "        postag = nltk.pos_tag(text)\n",
    "        tag_fd = nltk.FreqDist(tag for (word, tag) in postag)\n",
    "        print (dataset+\": \")\n",
    "        #print(tag_fd.most_common())\n",
    "        for word, frequency in tag_fd.most_common(50):\n",
    "            print(u'{}, {}'.format(word, frequency))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
